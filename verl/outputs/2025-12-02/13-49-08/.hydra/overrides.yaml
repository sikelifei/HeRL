- algorithm.adv_estimator=grpo
- +algorithm.enable_fuse=True
- data.train_files=/workspace/verl/hirdata/sample/train128.parquet
- data.val_files=/workspace/verl/hirdata/sample/test256.parquet
- data.train_batch_size=64
- data.max_prompt_length=4092
- data.max_response_length=1024
- data.filter_overlong_prompts=True
- data.truncation=error
- actor_rollout_ref.model.path=/workspace/model/Llama-3.2-3B-Instruct
- actor_rollout_ref.actor.optim.lr=1e-6
- actor_rollout_ref.model.use_remove_padding=True
- actor_rollout_ref.actor.ppo_mini_batch_size=64
- actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16
- actor_rollout_ref.actor.use_kl_loss=True
- actor_rollout_ref.actor.kl_loss_coef=0.001
- actor_rollout_ref.actor.kl_loss_type=low_var_kl
- actor_rollout_ref.actor.entropy_coeff=0
- actor_rollout_ref.model.enable_gradient_checkpointing=True
- actor_rollout_ref.actor.fsdp_config.param_offload=False
- actor_rollout_ref.actor.fsdp_config.optimizer_offload=False
- actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=16
- actor_rollout_ref.rollout.tensor_model_parallel_size=2
- actor_rollout_ref.rollout.name=vllm
- actor_rollout_ref.rollout.gpu_memory_utilization=0.4
- actor_rollout_ref.rollout.n=16
- actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=16
- actor_rollout_ref.ref.fsdp_config.param_offload=True
- algorithm.use_kl_in_reward=False
- +trainer.fuse_offpolicy.enable=True
- +trainer.fuse_offpolicy.max_pairs_per_prompt=100
- +trainer.fuse_offpolicy.max_jobs_per_step=1000
- +trainer.fuse_offpolicy.temperature=1.0
- +trainer.fuse_offpolicy.response_length=1024
- +trainer.fuse_offpolicy.do_sample=True
- trainer.critic_warmup=0
- trainer.val_before_train=False
- trainer.default_local_dir=/workspace/model/output/llama3
- trainer.logger=["console"]
- trainer.project_name=verl_llama3b_hir_offpolicy_fuse2
- trainer.experiment_name=llama3b_hir_offpolicy_fuse2
- custom_reward_function.path=/workspace/verl/verl/utils/reward_score/if_score/reward_compute.py
- custom_reward_function.name=compute_score
- data.reward_fn_key=data_source
- reward_model.reward_manager=naive
- trainer.n_gpus_per_node=4
- trainer.nnodes=1
- trainer.save_freq=-1
- trainer.test_freq=10
- trainer.total_epochs=3
